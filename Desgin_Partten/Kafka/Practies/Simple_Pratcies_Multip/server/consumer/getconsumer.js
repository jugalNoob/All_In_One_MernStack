const kafka = require("../client/client");
const connectDB = require("../db/conn");
const Register = require("../module/student"); // Optional: if used for lookup, not insertion
const os = require("os");

async function initConsumer() {
  const consumer = kafka.consumer({
    groupId: "user-consumer-group",
    // Optional for scalability tuning:
    // maxBytesPerPartition: 1048576, // 1MB per partition
  });

  try {
    // ğŸŸ¢ 1. Connect to MongoDB (if needed for lookups or caching)
    await connectDB();
    console.log("âœ… MongoDB connected");

    // ğŸŸ¢ 2. Connect to Kafka broker
    await consumer.connect();
    console.log(`âœ… Kafka Consumer connected on ${os.hostname()} [PID ${process.pid}]`);

    // ğŸŸ¢ 3. Subscribe to topic
    await consumer.subscribe({
      topic: "user-fetch-events",
      fromBeginning: false, // Set true only for replays/debugging
    });

    console.log("ğŸ“¡ Listening for messages on topic: user-fetch-events");

    // ğŸ”„ Cache in memory or forward to another system (e.g., Redis, analytics)
    const userCache = [];

    // ğŸš€ 4. Consume messages in **batches** (faster, more efficient)
    await consumer.run({
      autoCommit: false, // âœ… Use manual offset commits for full control
      eachBatch: async ({ batch, resolveOffset, heartbeat, commitOffsetsIfNecessary }) => {
        console.log(`ğŸ“¦ Processing batch: ${batch.messages.length} messages`);

        // ğŸš¨ Batch guard
        if (!batch.messages || batch.messages.length === 0) {
          return;
        }

        for (const message of batch.messages) {
          try {
            const value = message.value?.toString();
            if (!value) {
              console.warn("âš ï¸ Empty message value, skipping...");
              resolveOffset(message.offset);
              continue;
            }

            const parsed = JSON.parse(value);

            // âœ… Validate 'users' format
            if (!Array.isArray(parsed.users)) {
              console.warn("âš ï¸ Malformed payload (missing 'users' array):", value);
              resolveOffset(message.offset);
              continue;
            }

            console.log(`ğŸ‘¤ Received ${parsed.users.length} users from message`);

            // ğŸ§  Example: Process/cache data
            userCache.push(...parsed.users);

            // âœ… Optional: Use for email analytics, duplication check, metrics
            // const emails = parsed.users.map(u => u.email);

            // Optional DB lookup (disabled here for scale)
            // const exists = await Register.find({ email: { $in: emails } });

            // âœ… Mark message offset as processed
            resolveOffset(message.offset);
          } catch (err) {
            console.error("âŒ Failed to parse/process message:", err.message);
            resolveOffset(message.offset); // Still resolve to avoid blocking
          }
        }

        // âœ… Commit all resolved offsets for the batch
        await commitOffsetsIfNecessary();

        // ğŸ”„ Keep the session alive (avoid rebalance)
        await heartbeat();
      },
    });

    // ğŸ§¼ Graceful shutdown handler
    process.on("SIGINT", async () => {
      console.log("ğŸ›‘ SIGINT received, disconnecting Kafka consumer...");
      await consumer.disconnect();
      process.exit(0);
    });

  } catch (err) {
    console.error("âŒ Error during Kafka consumer init:", err.message);
    process.exit(1); // Fail fast in production
  }
}

// ğŸš€ Start the consumer
initConsumer();
